{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T21:08:10.214984Z",
     "start_time": "2019-02-12T21:08:07.931963Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T21:08:12.288684Z",
     "start_time": "2019-02-12T21:08:10.221109Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:01<00:00, 28969.44it/s]\n"
     ]
    }
   ],
   "source": [
    "def pathJoin(*args):\n",
    "    return os.path.abspath(os.path.join(*args))\n",
    "\n",
    "class BaseDataset(Dataset):\n",
    "\n",
    "    def __init__(self, directory, split='train', transforms=None):\n",
    "        self.datapoints = defaultdict(list)\n",
    "        self.split = split\n",
    "        self.directory = pathJoin(directory, split)\n",
    "        self.datapoints = self.loadDataset()\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datapoints)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        datapoint = self.loadDatapoint(idx)\n",
    "        return datapoint\n",
    "\n",
    "    def loadDatapoint(self, idx):\n",
    "        raise NotImplementedError('Function \"loadDatapoint\" is not implemented')\n",
    "\n",
    "    def loadDataset(self, name):\n",
    "        raise NotImplementedError('Function \"loadDataset\" is not implemented')\n",
    "\n",
    "class ImageNetDataset(BaseDataset):\n",
    "\n",
    "    def __init__(self, directory, split='train', transforms=None):\n",
    "        super().__init__(directory, split, transforms)\n",
    "        self.descriptions = self.loadDescriptions()\n",
    "        self.classes = self.loadClasses()\n",
    "        self.groundtruths = self.loadValidationGroundtruths() if split == 'val' else []\n",
    "\n",
    "    def loadDatapoint(self, idx):\n",
    "        filepath = self.datapoints[idx]\n",
    "        image = Image.open(filepath).convert('RGB')\n",
    "        if self.split == 'val':\n",
    "            groundtruth = self.groundtruths[idx]\n",
    "        elif self.split == 'train':\n",
    "            groundtruth = self.classes.index(filepath.split('/').pop().split('_')[0])\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        return (filepath, image, groundtruth, self.descriptions[groundtruth])\n",
    "\n",
    "    def loadDataset(self):\n",
    "        datapoints = []\n",
    "\n",
    "        dataset_file_list_filename = 'ilsvrc2012{}.txt'.format(self.split)\n",
    "        dataset_file_list_path = os.path.join(self.directory, dataset_file_list_filename)\n",
    "\n",
    "        with open(dataset_file_list_path, 'r') as dataset_file_list_file:\n",
    "            for line in tqdm(dataset_file_list_file, total=sum(1 for line in open(dataset_file_list_path))):\n",
    "                file_path = pathJoin(self.directory, self.sanitizeFilename(line))\n",
    "                datapoints.append(file_path)\n",
    "        \n",
    "        return datapoints\n",
    "    \n",
    "    def sanitizeFilename(self, filename):\n",
    "        return filename.replace('\"', '').strip()\n",
    "\n",
    "    def loadDescriptions(self):\n",
    "        descriptions = []\n",
    "\n",
    "        descriptions_filename = 'synsets_with_descriptions.txt'\n",
    "        descriptions_path = pathJoin(self.directory, '..', descriptions_filename)\n",
    "\n",
    "        with open(descriptions_path, 'r') as descriptions_file:\n",
    "            for line in descriptions_file:\n",
    "                description_breakdown = line.split(' ')\n",
    "                description_breakdown.pop(0)\n",
    "                description = ' '.join(description_breakdown).strip()\n",
    "                descriptions.append(description)\n",
    "\n",
    "        return descriptions\n",
    "\n",
    "    def loadValidationGroundtruths(self):\n",
    "        groundtruths = []\n",
    "\n",
    "        groundtruths_filename = 'validation_ground_truth.txt'\n",
    "        groundtruths_path = pathJoin(self.directory, '..', groundtruths_filename)\n",
    "\n",
    "        with open(groundtruths_path, 'r') as groundtruths_file:\n",
    "            for line in groundtruths_file:\n",
    "                groundtruth_breakdown = line.split(' ')\n",
    "                groundtruth_breakdown.pop(0)\n",
    "                groundtruth = ' '.join(groundtruth_breakdown).strip()\n",
    "                groundtruths.append(int(groundtruth))\n",
    "\n",
    "        return groundtruths\n",
    "\n",
    "    def loadClasses(self):\n",
    "        classes = []\n",
    "\n",
    "        classes_filename = 'synsets.txt'\n",
    "        classes_path = pathJoin(self.directory, '..', classes_filename)\n",
    "\n",
    "        with open(classes_path, 'r') as classes_file:\n",
    "            for line in classes_file:\n",
    "                classes.append(line.strip())\n",
    "\n",
    "        return classes\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                  std=[0.229, 0.224, 0.225])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "#     normalize\n",
    "])\n",
    "\n",
    "imagenet_dataset_path = os.path.join('datasets', 'imagenet')\n",
    "\n",
    "# imagenet_train_dataset = ImageNetDataset(imagenet_dataset_path, transforms=test_transforms)\n",
    "imagenet_val_dataset = ImageNetDataset(imagenet_dataset_path, split='val', transforms=test_transforms)\n",
    "# imagenet_test_dataset = ImageNetDataset(imagenet_dataset_path, split='test')\n",
    "\n",
    "# imagenet_train_loader = DataLoader(imagenet_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "imagenet_val_loader = DataLoader(imagenet_val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "# imagenet_test_loader = DataLoader(imagenet_test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T21:28:58.142981Z",
     "start_time": "2019-02-12T21:28:58.130067Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143, 'oystercatcher')\n",
      "(281, 'tabby cat')\n",
      "(282, 'tiger cat')\n",
      "(283, 'Persian cat')\n",
      "(284, 'Siamese cat')\n",
      "(358, 'European polecat')\n",
      "(484, 'catamaran')\n"
     ]
    }
   ],
   "source": [
    "indices = [i for i, s in enumerate(imagenet_val_dataset.descriptions) if 'cat' in s]\n",
    "cats = [ imagenet_val_dataset.descriptions[i] for i in indices ]\n",
    "for i in zip(indices, cats):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T17:29:11.610347Z",
     "start_time": "2018-12-26T17:17:18.588150Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cats = [ x for x in tqdm(imagenet_val_dataset) if x[2] == 281 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T17:29:11.625273Z",
     "start_time": "2018-12-26T17:29:11.617031Z"
    }
   },
   "outputs": [],
   "source": [
    "toImage = torchvision.transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T19:07:47.558004Z",
     "start_time": "2018-12-26T19:04:24.271527Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total = len(cats)\n",
    "row_size = 10\n",
    "\n",
    "for index, cat in enumerate(cats):\n",
    "    plt.figure(figsize=(100,100))\n",
    "    plt.subplot(total, 1, index+1)\n",
    "    plt.imshow(toImage(cat[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T16:56:44.072122Z",
     "start_time": "2018-12-25T16:56:41.007702Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "resnet50 = torchvision.models.resnet50(pretrained=True)\n",
    "resnet50.eval()\n",
    "\n",
    "toImage = torchvision.transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T14:16:22.579903Z",
     "start_time": "2019-01-07T14:16:21.682683Z"
    }
   },
   "outputs": [],
   "source": [
    "current_index = 0\n",
    "\n",
    "batch_accuracy = []\n",
    "individual_accuracy = []\n",
    "\n",
    "for index, batch in enumerate(imagenet_val_loader):\n",
    "    resnet50.cuda()\n",
    "    # batch\n",
    "    batch_images = batch[1] # index 1 has the images\n",
    "    b_output = resnet50(batch_images.cuda())\n",
    "    _, b_pred = b_output.topk(1, 1, True, True)\n",
    "#     print(batch[2])\n",
    "#     print(b_pred.cpu().squeeze())\n",
    "#     print(accuracy_score(batch[2], b_pred.cpu().squeeze()))\n",
    "    batch_accuracy.append(accuracy_score(batch[2], b_pred.cpu().squeeze()))\n",
    "    b_pred = b_pred[current_index] # prediction for first image\n",
    "    \n",
    "    # individually\n",
    "    single_image = batch[1][current_index]\n",
    "    single_image = single_image.unsqueeze(0) # include batch dimension\n",
    "    i_output = resnet50(single_image.cuda())\n",
    "    _, i_pred = i_output.topk(1, 1, True, True)\n",
    "    \n",
    "    print('{} (truth) -- {} (batch) -- {} (single)'.format(\n",
    "        batch[3][current_index],\n",
    "        imagenet_val_dataset.descriptions[b_pred.item()],\n",
    "        imagenet_val_dataset.descriptions[i_pred.item()]\n",
    "    ))\n",
    "    \n",
    "    print(batch_accuracy)\n",
    "\n",
    "#     assert i_pred.item() == b_pred.item(), 'Predictions are not the same'\n",
    "    if (index + 1) == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T16:56:24.117406Z",
     "start_time": "2018-12-25T16:56:24.041917Z"
    }
   },
   "outputs": [],
   "source": [
    "x1 = torch.tensor([ 65, 970, 230, 809, 516,  57, 334, 415, 674, 332, 109, 286, 370, 757,\n",
    "        595, 147, 108,  23, 478, 517, 334, 173, 948, 727,  23, 846, 270, 167,\n",
    "         55, 858, 324, 573, 150, 981, 586, 887,  32, 398, 777,  74, 516, 756,\n",
    "        129, 198, 256, 725, 565, 167, 717, 394,  92,  29, 844, 591, 358, 468,\n",
    "        259, 994, 872, 588, 474, 183, 107,  46])\n",
    "x2 = torch.tensor([ 65, 795, 230, 967, 520,  58, 334, 852, 674, 332, 109, 286, 370, 757,\n",
    "        595, 147,  76,  21, 478, 517, 334, 173, 948, 727,  23, 846, 270, 166,\n",
    "         55, 538, 324, 573, 360, 981, 586, 887,  26, 398, 777,  74, 431, 756,\n",
    "        129, 198, 256, 505, 565, 162, 717, 395,  92,  29, 844, 591, 359, 468,\n",
    "        259, 994, 840, 588, 474, 197, 107,  40])\n",
    "\n",
    "\n",
    "y1 = torch.tensor([ 65, 970, 230, 809, 516,  57, 334, 415, 674, 332, 109, 286, 370, 757,\n",
    "        595, 147, 108,  23, 478, 517, 334, 173, 948, 727,  23, 846, 270, 167,\n",
    "         55, 858, 324, 573, 150, 981, 586, 887,  32, 398, 777,  74, 516, 756,\n",
    "        129, 198, 256, 725, 565, 167, 717, 394,  92,  29, 844, 591, 358, 468,\n",
    "        259, 994, 872, 588, 474, 183, 107,  46])\n",
    "y2 = torch.tensor([ 65, 795, 231, 659, 431,  65, 334, 529, 532, 332, 109, 286, 370, 757,\n",
    "        595, 147,  29,  21, 478, 517, 334, 354, 948, 727,  23, 846, 270, 166,\n",
    "         64, 832, 324, 573,  33, 981, 586, 887,  26, 398, 529, 126, 431, 713,\n",
    "        129, 196, 256, 968, 565, 180, 717,  69, 395,  29, 844, 591, 516, 468,\n",
    "        259, 994, 840, 681, 841, 197,   5,  40])\n",
    "\n",
    "print(accuracy_score(x1, x2))\n",
    "print(accuracy_score(y1, y2))\n",
    "\n",
    "print(torch.sum(x1 == y1))\n",
    "print(torch.sum(x1 == x2))\n",
    "print(torch.sum(y1 == y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T14:23:34.807511Z",
     "start_time": "2019-01-14T14:23:17.928784Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "values_directory = os.path.join('results', 'values')\n",
    "values_files = os.listdir(values_directory)\n",
    "for value_file in values_files:\n",
    "    true_class, predicted_class, imagenet_val_index = value_file.split('.')[0].split('-')\n",
    "    old_path = os.path.join(values_directory, value_file)\n",
    "    new_path = os.path.join(values_directory, '{}-{}-{}.pickle'.format(imagenet_val_index, true_class, predicted_class))\n",
    "    print('{} -> {}'.format(old_path, new_path))\n",
    "#     os.rename(old_path, new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T14:23:34.837703Z",
     "start_time": "2019-01-14T14:23:34.814364Z"
    }
   },
   "outputs": [],
   "source": [
    "values_directory = os.path.join('results', 'plots')\n",
    "values_files = os.listdir(values_directory)\n",
    "for value_file in values_files:\n",
    "    true_class, predicted_class, imagenet_val_index = value_file.split('.')[0].split('-')\n",
    "    old_path = os.path.join(values_directory, value_file)\n",
    "    new_path = os.path.join(values_directory, '{}-{}-{}.png'.format(imagenet_val_index, true_class, predicted_class))\n",
    "    print('{} -> {}'.format(old_path, new_path))\n",
    "#     os.rename(old_path, new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
